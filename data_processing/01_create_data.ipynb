{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import configparser\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cofig = configparser.RawConfigParser()\n",
    "cofig.read('config.ini')\n",
    "url = cofig['scrap_url']['url']\n",
    "product_table_path = cofig['intermediate_path']['product_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_browser(path):\n",
    "    '''\n",
    "    It will create a Chrome driver\n",
    "\n",
    "    Parameter:\n",
    "    - path: url of the website\n",
    "\n",
    "    Return:\n",
    "    - driver: Chrome driver \n",
    "    '''\n",
    "    try:\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(path)\n",
    "\n",
    "        time.sleep(10) # Wait for page to load\n",
    "        \n",
    "        driver.find_element(By.XPATH, '//button[text() =\"Ask Me Later\"]').click()\n",
    "\n",
    "        print('Driver created successfully')\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "def webpage_scroll(driver, scroll_value=1000):\n",
    "    '''\n",
    "    This function scroll the webpage.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: Chrome driver\n",
    "    - scroll_value: number of time to scrool the web-page\n",
    "\n",
    "    Return:\n",
    "    - driver: Chrome driver after scrolling\n",
    "    '''\n",
    "    try:\n",
    "        for _ in tqdm(range(scroll_value), desc='Scrolling Products'):\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(3)  # Allow images to load\n",
    "\n",
    "            try:\n",
    "                show_more_button = driver.find_element(By.XPATH, '//button[text()=\"Show More Products\"]')\n",
    "                if show_more_button.is_displayed():\n",
    "                    show_more_button.click()\n",
    "            except:\n",
    "                pass  # No button found, continue scrolling\n",
    "\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "def extract_product_href(product_element):\n",
    "    '''\n",
    "    Extracts the product name and href from the anchor tag.\n",
    "    '''\n",
    "    product_href = 'No Link Found'\n",
    "    product_name = product_element.get_attribute('title')  # Extract product title\n",
    "    try:\n",
    "        product_href = product_element.get_attribute('href')  # Extract href\n",
    "\n",
    "        # Ensure absolute URL if href is relative\n",
    "        if product_href and product_href.startswith('//'):\n",
    "            product_href = 'https:' + product_href\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return product_name, product_href\n",
    "\n",
    "def collect_data(driver):\n",
    "    '''\n",
    "    Extracts all product data by dynamically scrolling until no new products load.\n",
    "    '''\n",
    "    product_data = set()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"ProductModule__aTag\")))\n",
    "\n",
    "        product_cards = driver.find_elements(By.CLASS_NAME, \"ProductModule__aTag\")\n",
    "\n",
    "        # Extract product data in parallel\n",
    "        with ThreadPoolExecutor(max_workers=min(10, len(product_cards))) as executor:\n",
    "            results = list(tqdm(\n",
    "                executor.map(extract_product_href, product_cards),\n",
    "                desc=\"Extracting Product Links\",\n",
    "                total=len(product_cards),\n",
    "                unit=\"product\",\n",
    "                leave=False\n",
    "            ))\n",
    "\n",
    "        product_data.update((name, link) for name, link in results if link != 'No Link Found')\n",
    "\n",
    "        final_product_data = list(product_data)\n",
    "        print(f'\\nTotal Products Extracted: {len(final_product_data)}')\n",
    "        return final_product_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return list(product_data) \n",
    "\n",
    "    finally:\n",
    "        driver.quit() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrolling Products: 100%|██████████| 150/150 [1:12:02<00:00, 28.81s/it]\n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Products Extracted: 6036\n"
     ]
    }
   ],
   "source": [
    "driver = open_browser(path=url)\n",
    "driver = webpage_scroll(driver=driver, scroll_value=150)\n",
    "product_data = collect_data(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Products: 6036\n"
     ]
    }
   ],
   "source": [
    "scrap_data = pd.DataFrame(product_data, columns=['product_name', 'product_url'])\n",
    "image_data = scrap_data[~(scrap_data['product_url']=='No Image Found')]\n",
    "print(f'Total Products: {len(image_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_data = pd.DataFrame(product_data, columns=['product_name', 'product_url'])\n",
    "image_data = scrap_data[~(scrap_data['product_url']=='No Image Found')]\n",
    "\n",
    "pattern = r'mp\\d+'\n",
    "image_data['product_id'] = image_data['product_url'].apply(lambda x: re.findall(pattern, x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_data.to_csv(product_table_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
